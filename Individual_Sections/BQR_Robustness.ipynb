{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains code to compare the effect of label noise on BQR and BCE losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "CVwEsLH4jcC4",
    "outputId": "3ab8e382-a1f6-4f3a-d9f0-eebffe98bce9"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data_utils\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) \n",
    "\n",
    "Scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "atputHGP0CH8"
   },
   "outputs": [],
   "source": [
    "def create_xy(dataset, attribute_columns, target_column, delim, split_ratio, ditch_head=True):\n",
    "    with open(dataset, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    if ditch_head:\n",
    "        lines = lines[1:]\n",
    "    X = []\n",
    "    Y = []\n",
    "    for line in lines:\n",
    "        while len(line) > 0 and line[-1] == \"\\n\":\n",
    "            line = line[:len(line)-1]\n",
    "        split_array = line.split(delim)\n",
    "        all_columns = []\n",
    "        for value in split_array:\n",
    "            if value !=\"\" and value !=\" \":\n",
    "                all_columns.append(value)\n",
    "        if len(all_columns)==0:\n",
    "            break\n",
    "        point = []\n",
    "        for i in attribute_columns:\n",
    "            point.append(float(all_columns[i]))\n",
    "        X.append(point)\n",
    "        Y.append(float(all_columns[target_column]))\n",
    "    X_arr = np.asarray(X)\n",
    "    Scaler.fit(X_arr)\n",
    "    X_arr = Scaler.transform(X_arr)\n",
    "    Y_arr = np.asarray(Y)\n",
    "    thresh = np.median(Y_arr)\n",
    "    Y_arr_binary = np.where(Y_arr<=0,0,1)\n",
    "    unique, counts = np.unique(Y_arr_binary, return_counts=True)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_arr, Y_arr_binary, test_size = split_ratio)\n",
    "    return x_train, x_test, y_train, y_test, Y_arr, X_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O18-3vKlypZb"
   },
   "outputs": [],
   "source": [
    "# Flips 100*ratio% of the labels in target\n",
    "\n",
    "def corrupt(target, ratio):\n",
    "    result = target.copy()\n",
    "    indices = np.random.choice(len(target),int(len(target)*ratio),replace=False)\n",
    "    for i in indices:\n",
    "        result[i] = np.abs(target[i]-1)    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "_0RSBLYOknVZ",
    "outputId": "283deb29-148c-48e7-c87e-a4e62676abe5"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(111)\n",
    "\n",
    "\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, indim):\n",
    "        super(Network,self).__init__()\n",
    "        self.l1 = nn.Linear(indim,100)\n",
    "        self.l2 = nn.Linear(100,10)\n",
    "        self.l3 = nn.Linear(10,9)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.l1(x))\n",
    "        x = F.leaky_relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class BCENetwork(nn.Module):\n",
    "    def __init__(self, indim):\n",
    "        super(BCENetwork,self).__init__()\n",
    "        self.l1 = nn.Linear(indim,100)\n",
    "        self.l2 = nn.Linear(100,10)\n",
    "        self.l3 = nn.Linear(10,1)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.l1(x))\n",
    "        x = F.leaky_relu(self.l2(x))\n",
    "        x = F.sigmoid(self.l3(x))\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TO98SBinR6-z"
   },
   "outputs": [],
   "source": [
    "# Loss and Accuracy Computation functions\n",
    "\n",
    "def cumLaplaceDistribution(y_pred,mean,standard_deviation,all_qs):\n",
    "    term1 = ((1-all_qs) * (y_pred - mean))/standard_deviation\n",
    "    term1.clamp_(max = 0) # Prevents NaN - Only one of term 1 or 2 is used, whichever is -ve\n",
    "    lesser_term = all_qs * torch.exp(term1)\n",
    "    term2 = (-1.0 * all_qs * (y_pred - mean))/standard_deviation\n",
    "    term2.clamp_(max = 0) # Again, Prevents NaN\n",
    "    greater_term = 1 - ((1-all_qs) * torch.exp(term2))\n",
    "    mean_tensor = torch.ones_like(mean)\n",
    "    y_mask = torch.div(y_pred,mean_tensor)\n",
    "    y_mask[y_pred >= mean] = 1.0\n",
    "    y_mask[y_pred < mean] = 0.0\n",
    "    return ((1 - y_mask) * lesser_term )+  (y_mask * greater_term)\n",
    "\n",
    "\n",
    "def logLikelihoodLoss(y_true,y_pred,mean,standard_deviation,all_qs):\n",
    "    new_pred = y_pred\n",
    "    prob = cumLaplaceDistribution(0.0,mean = new_pred,\n",
    "                                  standard_deviation = standard_deviation,all_qs = all_qs)\n",
    "    prob.clamp_(min = 1e-7,max = 1 - 1e-7)\n",
    "    if_one = y_true * torch.log(1 - prob)\n",
    "    if_zero = (1 - y_true) * torch.log(prob)\n",
    "    final_loss = - 1 * torch.mean(if_one + if_zero)\n",
    "    return final_loss\n",
    "\n",
    "def customLoss(y_true, y_pred, mean, standard_deviation, all_qs, penalty):\n",
    "    ind_losses = []\n",
    "    for i,j in enumerate(all_qs):\n",
    "        single_quantile_loss = logLikelihoodLoss(y_true[:,0],y_pred[:,i] ,\n",
    "                                                 mean, standard_deviation, j)\n",
    "        ind_losses.append(single_quantile_loss)\n",
    "    zero = torch.Tensor([0]).to(device)\n",
    "    dummy1 = y_pred[:,1:] - y_pred[:,:-1]\n",
    "    dummy2 = penalty * torch.mean(torch.max(zero,-1.0 * dummy1))\n",
    "    total_loss  = torch.mean(torch.stack(ind_losses)) +dummy2\n",
    "    return total_loss\n",
    "\n",
    "def customTestPred(y_pred,mean,standard_deviation,all_qs,batch_size = 1):\n",
    "    acc = []\n",
    "    cdfs = []\n",
    "    val = (y_pred - mean)/standard_deviation \n",
    "    \n",
    "    for xx in range(batch_size):\n",
    "        if(y_pred < mean[xx]):\n",
    "            lesser_term = all_qs * torch.exp((1.0 - all_qs) * torch.tensor(val[xx], dtype=torch.double)) \n",
    "            # Typecast above needed for some versions of torch\n",
    "            lesser_term  = 1 - lesser_term\n",
    "            cdfs.append(lesser_term.item())\n",
    "            if(lesser_term.item() >= 0.5):\n",
    "                acc.append([1])\n",
    "            else:\n",
    "                acc.append([0])\n",
    "        \n",
    "        elif(y_pred >= mean[xx]):\n",
    "            greater_term = 1.0 - ((1.0-all_qs) * torch.exp(-1.0 * all_qs * torch.tensor(val[xx], dtype=torch.double)))\n",
    "            # Typecast above needed for some versions of torch\n",
    "            greater_term = 1 - greater_term\n",
    "            cdfs.append(greater_term.item())\n",
    "            if(greater_term.item() >= 0.5):\n",
    "                acc.append([1])\n",
    "            else:\n",
    "                acc.append([0])\n",
    "    return torch.Tensor(acc).to(device).reshape(-1,1),torch.Tensor(cdfs).to(device).reshape(-1,1)\n",
    "\n",
    "def acc_tests(test_preds,test_labels):\n",
    "    test_preds = np.array(test_preds).reshape(-1,1)\n",
    "    test_labels = np.array(test_labels).reshape(-1,1)\n",
    "    cdfs_acc,_ = customTestPred(0,test_preds,standard_deviation = 1,all_qs = torch.Tensor([0.5]),\n",
    "                                batch_size = test_preds.shape[0])\n",
    "\n",
    "    count = 0\n",
    "    for i,j in zip(cdfs_acc,test_labels):\n",
    "        if(i.item() == j[0]):\n",
    "            count += 1\n",
    "    return count/test_labels.shape[0]\n",
    "\n",
    "\n",
    "def bce_test(preds, labels):\n",
    "    count = 0\n",
    "    for i,j in zip(preds,labels):\n",
    "        if i < 0.5:\n",
    "            prediction = 0\n",
    "        else:\n",
    "            prediction = 1\n",
    "        if(prediction == j):\n",
    "            count += 1\n",
    "    return count/len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gb9suXC1VBBZ"
   },
   "outputs": [],
   "source": [
    "# Training and Testing Methods\n",
    "\n",
    "def train(model,optimizer,loader,epochs, verbose=False):\n",
    "    train_preds_Q = []\n",
    "    train_labels = []\n",
    "    model.train()\n",
    "    \n",
    "    for i,j in enumerate(loader):\n",
    "        inputs,labels = j[0],j[1]\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        op_qs = model(inputs)\n",
    "        lossQ = customLoss(labels.reshape(-1,1),op_qs, mean_is,std_is,all_qs,penalty)\n",
    "        lossQ.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        for lag in op_qs[:,4].detach().reshape(-1,1):\n",
    "            train_preds_Q.append(lag.item())\n",
    "        for lag in labels.reshape(-1,1):\n",
    "            train_labels.append(lag.item())\n",
    "            \n",
    "    acc_is_Q = acc_tests(train_preds_Q,train_labels)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"[%d/%d] Train Acc Q : %f \"%(epochs,total_epochs,acc_is_Q))\n",
    "    return acc_is_Q\n",
    "\n",
    "def test(model,loader,epochs,verbose=False):\n",
    "    model.eval()\n",
    "    test_preds_Q = []\n",
    "    test_preds_bce = []\n",
    "    test_labels = []\n",
    "    with torch.no_grad():\n",
    "        for i,j in enumerate(loader):\n",
    "            inputs,labels = j[0],j[1]\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            op_qs = model(inputs)\n",
    "            \n",
    "            for lag in op_qs[:,4].detach().reshape(-1,1):\n",
    "                test_preds_Q.append(lag.item())\n",
    "            for lag in labels.reshape(-1,1):\n",
    "                test_labels.append(lag.item())\n",
    "                \n",
    "    acc_is_Q = acc_tests(test_preds_Q,test_labels)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"[%d/%d] Test Acc Q : %f  \"%(epochs,total_epochs,acc_is_Q))\n",
    "    return acc_is_Q\n",
    "\n",
    "def train_bce(model,opt,loader,epochs,verbose=False):\n",
    "    bce_loss = nn.BCELoss()\n",
    "    model.train()\n",
    "    preds = []\n",
    "    true_labels = []\n",
    "    for i,j in enumerate(loader):\n",
    "        inputs,labels = j[0],j[1]\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        opt.zero_grad()\n",
    "        op = model(inputs)\n",
    "        loss = bce_loss(op,labels.reshape(-1,1))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        for lag in op.detach().reshape(-1,1):\n",
    "            preds.append(lag.item())\n",
    "        for lag in labels.reshape(-1,1):\n",
    "            true_labels.append(lag.item())\n",
    "    acc = bce_test(preds, true_labels)\n",
    "    if verbose:\n",
    "        print(\"[%d/%d] Test Acc Q : %f  \"%(epochs,total_epochs,acc))\n",
    "    return acc\n",
    "\n",
    "def test_bce(model,loader,epochs,verbose=False):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    true_labels = []\n",
    "    for i,j in enumerate(loader):\n",
    "        inputs,labels = j[0],j[1]\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        op = model(inputs)\n",
    "        for lag in op.detach().reshape(-1,1):\n",
    "            preds.append(lag.item())\n",
    "        for lag in labels.reshape(-1,1):\n",
    "            true_labels.append(lag.item())\n",
    "    acc = bce_test(preds, true_labels)\n",
    "    if verbose:\n",
    "        print(\"[%d/%d] Test Acc Q : %f  \"%(epochs,total_epochs,acc))\n",
    "    return acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u4-Kb18YeBHC"
   },
   "outputs": [],
   "source": [
    "def quantileCDF(x, tau):\n",
    "    if x>0:\n",
    "        return 1 - tau*np.exp((tau-1)*x)\n",
    "    else:\n",
    "        return (1 - tau)*np.exp(tau*x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Device: cpu\n"
     ]
    }
   ],
   "source": [
    "batch_is = 64\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.deterministic=True\n",
    "print(\"Torch Device:\",device)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5XuwYZfoiFba"
   },
   "outputs": [],
   "source": [
    "# Adjust the dataset details here. Refer the dataset_params.txt file for the specifics of each dataset\n",
    "dataset = '../Datasets/Classification/WBC.csv'\n",
    "x_cols = list(range(1,10))\n",
    "y_col = 10\n",
    "separator = \",\"\n",
    "remove_head = True\n",
    "split_ratio = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# General Control Parameters for the Quantile loss. Need not be changed\n",
    "lr_is = 1e-2\n",
    "mean_is = 0\n",
    "std_is = 1\n",
    "penalty = 1\n",
    "alpha = 0.0\n",
    "\n",
    "# Tau tensor\n",
    "all_qs = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "all_qs = torch.Tensor(all_qs).to(device)\n",
    "all_qs = all_qs.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment Control Parameters\n",
    "total_runs = 10    # Number of times to run the experiment\n",
    "total_epochs = 20  # No of training epochs per run\n",
    "verbosity = False  # Toggle verbose training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 991
    },
    "colab_type": "code",
    "id": "o_dkljAxCpHb",
    "outputId": "5b1b2c7f-9521-471f-ec72-441a4fce51e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise Ratio 0\n",
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n",
      "Iteration: 5\n",
      "Iteration: 6\n",
      "Iteration: 7\n",
      "Iteration: 8\n",
      "Iteration: 9\n",
      "Iteration: 10\n",
      "----------\n",
      "Noise Ratio 0.1\n",
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n",
      "Iteration: 5\n",
      "Iteration: 6\n",
      "Iteration: 7\n",
      "Iteration: 8\n",
      "Iteration: 9\n",
      "Iteration: 10\n",
      "----------\n",
      "Noise Ratio 0.2\n",
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n",
      "Iteration: 5\n",
      "Iteration: 6\n",
      "Iteration: 7\n",
      "Iteration: 8\n",
      "Iteration: 9\n",
      "Iteration: 10\n",
      "----------\n",
      "Noise Ratio 0.3\n",
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n",
      "Iteration: 5\n",
      "Iteration: 6\n",
      "Iteration: 7\n",
      "Iteration: 8\n",
      "Iteration: 9\n",
      "Iteration: 10\n",
      "----------\n",
      "Noise Ratio 0.4\n",
      "Iteration: 1\n",
      "Iteration: 2\n",
      "Iteration: 3\n",
      "Iteration: 4\n",
      "Iteration: 5\n",
      "Iteration: 6\n",
      "Iteration: 7\n",
      "Iteration: 8\n",
      "Iteration: 9\n",
      "Iteration: 10\n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "nr = [0,0.1,0.2,0.3,0.4] # Noise Ratios\n",
    "total_epochs = 20\n",
    "iterations = 10\n",
    "median_average = []\n",
    "bce_average = []\n",
    "pred_range_average = []\n",
    "\n",
    "for noise_ratio in nr:\n",
    "    print(\"Noise Ratio\",noise_ratio)\n",
    "    bce_total = 0\n",
    "    median_total = 0\n",
    "    pred_delta_total = 0\n",
    "    \n",
    "    for iter in range(iterations):\n",
    "        print(\"Iteration:\",iter+1)\n",
    "        X_train,X_val,y_train_legit,y_val, data_Y, data_X = create_xy(dataset, x_cols, y_col, \",\", split_ratio)\n",
    "        y_train = corrupt(y_train_legit, noise_ratio)\n",
    "        X_train = torch.Tensor(X_train)\n",
    "        y_train = torch.Tensor(y_train)\n",
    "        X_val = torch.Tensor(X_val)\n",
    "        y_val = torch.Tensor(y_val)\n",
    "        train_dataset = data_utils.TensorDataset(X_train, y_train)\n",
    "        test_dataset = data_utils.TensorDataset(X_val, y_val)\n",
    "        train_loader = data_utils.DataLoader(train_dataset, batch_size =64, pin_memory=True,shuffle=True,num_workers = 1)\n",
    "        test_loader = data_utils.DataLoader(test_dataset,batch_size =64,pin_memory=True,shuffle = False,num_workers = 1)\n",
    "        \n",
    "        indim = X_train.shape[1]\n",
    "        model = Network(indim)\n",
    "        model = model.to(device)\n",
    "\n",
    "        bce_model = BCENetwork(indim)\n",
    "        bce_model = bce_model.to(device)\n",
    "\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr = lr_is)\n",
    "        bce_opt = torch.optim.Adam(bce_model.parameters(), lr=lr_is)\n",
    "        \n",
    "        for i in range(total_epochs):\n",
    "            acc_train_med = train(model,optimizer, train_loader,i,verbosity)\n",
    "            acc_train_bce = train_bce(bce_model,bce_opt, train_loader,i,verbosity)\n",
    "            \n",
    "    \n",
    "        X_cov = torch.Tensor(data_X)\n",
    "        y_cov = torch.Tensor(data_Y)\n",
    "\n",
    "        cov_dataset = data_utils.TensorDataset(X_cov, y_cov)\n",
    "        cov_loader = data_utils.DataLoader(cov_dataset, batch_size =64, pin_memory=True,shuffle=True,num_workers = 1)\n",
    "        preds_Q = []\n",
    "        prob_Q = []\n",
    "        preds_bce = []\n",
    "        true_labels = []\n",
    "        for i,j in enumerate(cov_loader):\n",
    "            inputs,labels = j[0],j[1]\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            op_qs = model(inputs)\n",
    "            op_bce = bce_model(inputs)\n",
    "            for lag in op_qs[:,4].detach().reshape(-1,1):\n",
    "                preds_Q.append(lag.item())\n",
    "                prob_Q.append(quantileCDF(lag.item(),0.5))\n",
    "            for lag in op_bce.detach().reshape(-1,1):\n",
    "                preds_bce.append(lag.item())\n",
    "            for lag in labels.reshape(-1,1):\n",
    "                true_labels.append(lag.item()) \n",
    "            \n",
    "        bce_total += bce_test(preds_bce, true_labels)/iterations\n",
    "        median_total += acc_tests(preds_Q, true_labels)/iterations\n",
    "\n",
    "    median_average.append(median_total)\n",
    "    bce_average.append(bce_total)\n",
    "    print(\"----------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jgteuJijiKEu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise Ratio | 0.000  | 0.100  | 0.200  | 0.300  | 0.400  | \n",
      "BCE         | 0.098  | 0.097  | 0.095  | 0.093  | 0.087  | \n",
      "BQR         | 0.098  | 0.097  | 0.095  | 0.094  | 0.095  | \n"
     ]
    }
   ],
   "source": [
    "nr_header = \"Noise Ratio | \"\n",
    "bce_res   = \"BCE         | \"\n",
    "bqr_res   = \"BQR         | \"\n",
    "endstring = \" | \"\n",
    "for i in range(len(nr)):\n",
    "    bce_res += \"{:.3f} \".format(bce_average[i]) + endstring\n",
    "    bqr_res += \"{:.3f} \".format(median_average[i]) + endstring\n",
    "    nr_header += \"{:.3f} \".format(nr[i]) + endstring\n",
    "\n",
    "print(nr_header)\n",
    "print(bce_res)\n",
    "print(bqr_res)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "MedianRobustness.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
