{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "xxNZ41a2tone"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data_utils\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "#import shap\n",
    "import pandas as pd\n",
    "import time\n",
    "from IPython.display import HTML\n",
    "import random\n",
    "from sklearn import datasets\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) # Prevents activation function warning messages \n",
    "warnings.filterwarnings(\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "Scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "qQjebQaItz00"
   },
   "outputs": [],
   "source": [
    "# Dataset Splitter function\n",
    "def create_xy(dataset, attribute_columns, target_column, delim, split_ratio, threshold, Scaler,ditch_head=False):\n",
    "    with open(dataset, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        \n",
    "    if ditch_head:\n",
    "        lines = lines[1:]\n",
    "    X = []\n",
    "    Y = []\n",
    "    \n",
    "    for line in lines:\n",
    "        while len(line) > 0 and line[-1] == \"\\n\":\n",
    "            line = line[:len(line)-1]\n",
    "        split_array = line.split(delim)\n",
    "        all_columns = []\n",
    "        for value in split_array:\n",
    "            if value !=\"\" and value !=\" \":\n",
    "                all_columns.append(value)\n",
    "        if len(all_columns)==0:\n",
    "            break\n",
    "        point = []\n",
    "        for i in attribute_columns:\n",
    "            point.append(float(all_columns[i]))\n",
    "        try:\n",
    "            Y.append(float(all_columns[target_column]))\n",
    "            X.append(point)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    X_arr = np.asarray(X)\n",
    "    Scaler.fit(X_arr)\n",
    "    X_arr = Scaler.transform(X_arr)\n",
    "    Y_arr = np.asarray(Y)\n",
    "    thresh = np.median(Y_arr)\n",
    "    Y_arr_binary = np.where(Y_arr<=threshold,0,1)\n",
    "    unique, counts = np.unique(Y_arr_binary, return_counts=True)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_arr, Y_arr_binary, test_size = split_ratio)\n",
    "    \n",
    "    return x_train, x_test, y_train, y_test, Y_arr, X_arr, threshold, Y_arr_binary\n",
    "\n",
    "\n",
    "# Loss and Accuracy Computation functions\n",
    "\n",
    "def cumLaplaceDistribution(y_pred,mean,standard_deviation,all_qs):\n",
    "    term1 = ((1-all_qs) * (y_pred - mean))/standard_deviation\n",
    "    term1.clamp_(max = 0) # Prevents NaN - Only one of term 1 or 2 is used, whichever is -ve\n",
    "    lesser_term = all_qs * torch.exp(term1)\n",
    "    term2 = (-1.0 * all_qs * (y_pred - mean))/standard_deviation\n",
    "    term2.clamp_(max = 0) # Again, Prevents NaN\n",
    "    greater_term = 1 - ((1-all_qs) * torch.exp(term2))\n",
    "    mean_tensor = torch.ones_like(mean)\n",
    "    y_mask = torch.div(y_pred,mean_tensor)\n",
    "    y_mask[y_pred >= mean] = 1.0\n",
    "    y_mask[y_pred < mean] = 0.0\n",
    "    return ((1 - y_mask) * lesser_term )+  (y_mask * greater_term)\n",
    "\n",
    "\n",
    "def logLikelihoodLoss(y_true,y_pred,mean,standard_deviation,all_qs):\n",
    "    new_pred = y_pred\n",
    "    prob = cumLaplaceDistribution(0.0,mean = new_pred,\n",
    "                                  standard_deviation = standard_deviation,all_qs = all_qs)\n",
    "    prob.clamp_(min = 1e-7,max = 1 - 1e-7)\n",
    "    if_one = y_true * torch.log(1 - prob)\n",
    "    if_zero = (1 - y_true) * torch.log(prob)\n",
    "    final_loss = - 1 * torch.mean(if_one + if_zero)\n",
    "    return final_loss\n",
    "\n",
    "def customLoss(y_true, y_pred, mean, standard_deviation, all_qs, penalty):\n",
    "    ind_losses = []\n",
    "    for i,j in enumerate(all_qs):\n",
    "        single_quantile_loss = logLikelihoodLoss(y_true[:,0],y_pred[:,i] ,\n",
    "                                                 mean, standard_deviation, j)\n",
    "        ind_losses.append(single_quantile_loss)\n",
    "    zero = torch.Tensor([0]).to(device)\n",
    "    dummy1 = y_pred[:,1:] - y_pred[:,:-1]\n",
    "    dummy2 = penalty * torch.mean(torch.max(zero,-1.0 * dummy1))\n",
    "    total_loss  = torch.mean(torch.stack(ind_losses)) +dummy2\n",
    "    return total_loss\n",
    "\n",
    "def customTestPred(y_pred,mean,standard_deviation,all_qs,batch_size = 1):\n",
    "    acc = []\n",
    "    cdfs = []\n",
    "    val = (y_pred - mean)/standard_deviation \n",
    "    \n",
    "    for xx in range(batch_size):\n",
    "        if(y_pred < mean[xx]):\n",
    "            lesser_term = all_qs * torch.exp((1.0 - all_qs) * torch.tensor(val[xx], dtype=torch.double)) \n",
    "            # Typecast above needed for some versions of torch\n",
    "            lesser_term  = 1 - lesser_term\n",
    "            cdfs.append(lesser_term.item())\n",
    "            if(lesser_term.item() >= 0.5):\n",
    "                acc.append([1])\n",
    "            else:\n",
    "                acc.append([0])\n",
    "        \n",
    "        elif(y_pred >= mean[xx]):\n",
    "            greater_term = 1.0 - ((1.0-all_qs) * torch.exp(-1.0 * all_qs * torch.tensor(val[xx], dtype=torch.double)))\n",
    "            # Typecast above needed for some versions of torch\n",
    "            greater_term = 1 - greater_term\n",
    "            cdfs.append(greater_term.item())\n",
    "            if(greater_term.item() >= 0.5):\n",
    "                acc.append([1])\n",
    "            else:\n",
    "                acc.append([0])\n",
    "    return torch.Tensor(acc).to(device).reshape(-1,1),torch.Tensor(cdfs).to(device).reshape(-1,1)\n",
    "\n",
    "def acc_tests(test_preds,test_labels):\n",
    "    test_preds = np.array(test_preds).reshape(-1,1)\n",
    "    test_labels = np.array(test_labels).reshape(-1,1)\n",
    "    cdfs_acc,_ = customTestPred(0,test_preds,standard_deviation = 1,all_qs = torch.Tensor([0.5]),\n",
    "                                batch_size = test_preds.shape[0])\n",
    "\n",
    "    count = 0\n",
    "    for i,j in zip(cdfs_acc,test_labels):\n",
    "        if(i.item() == j[0]):\n",
    "            count += 1\n",
    "    return count/test_labels.shape[0]\n",
    "\n",
    "# Training and Testing Methods\n",
    "\n",
    "def train(model,optimizer,loader,epochs,verbose=False):\n",
    "    train_preds_Q = []\n",
    "    train_labels = []\n",
    "    model.train()\n",
    "    \n",
    "    for i,j in enumerate(loader):\n",
    "        inputs,labels = j[0],j[1]\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        op_qs = model(inputs)\n",
    "        lossQ = customLoss(labels.reshape(-1,1),op_qs, mean_is,std_is,all_qs,penalty)\n",
    "        lossQ.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        for lag in op_qs[:,4].detach().reshape(-1,1):\n",
    "            train_preds_Q.append(lag.item())\n",
    "        for lag in labels.reshape(-1,1):\n",
    "            train_labels.append(lag.item())\n",
    "            \n",
    "    acc_is_Q = acc_tests(train_preds_Q,train_labels)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"[%d/%d] Train Acc Q : %f \"%(epochs,total_epochs,acc_is_Q))\n",
    "    return acc_is_Q\n",
    "\n",
    "def test(model,loader,epochs,verbose=False):\n",
    "    model.eval()\n",
    "    test_preds_Q = []\n",
    "    test_preds_bce = []\n",
    "    test_labels = []\n",
    "    with torch.no_grad():\n",
    "        for i,j in enumerate(loader):\n",
    "            inputs,labels = j[0],j[1]\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            op_qs = model(inputs)\n",
    "            \n",
    "            for lag in op_qs[:,4].detach().reshape(-1,1):\n",
    "                test_preds_Q.append(lag.item())\n",
    "            for lag in labels.reshape(-1,1):\n",
    "                test_labels.append(lag.item())\n",
    "                \n",
    "    acc_is_Q = acc_tests(test_preds_Q,test_labels)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"[%d/%d] Test Acc Q : %f  \"%(epochs,total_epochs,acc_is_Q))\n",
    "    return acc_is_Q\n",
    "\n",
    "def quantileCDF(x, tau):\n",
    "    if x>0:\n",
    "        return 1 - tau*np.exp((tau-1)*x)\n",
    "    else:\n",
    "        return (1 - tau)*np.exp(tau*x)\n",
    "    \n",
    "def normalize(arr, mean, std):\n",
    "    return (arr-mean)/std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0Gbw-3j7uqrR",
    "outputId": "c3fe72e0-05ae-4d9b-d7b3-a29c3d20f569"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# These are standard control variables\n",
    "batch_is = 32\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.deterministic=True\n",
    "print(\"Torch Device:\",device)\n",
    "torch.set_default_dtype(torch.double) # Prevents bugs in certain PyTorch versions from showing up\n",
    "\n",
    "# General Control Parameters for the Quantile loss. Need not be changed\n",
    "lr_is = 1e-2\n",
    "mean_is = 0\n",
    "std_is = 1\n",
    "penalty = 1\n",
    "alpha = 0.0\n",
    "\n",
    "# Tau tensor\n",
    "all_qs = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "all_qs = torch.Tensor(all_qs).to(device)\n",
    "all_qs = all_qs.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "dCz352GYuu2a"
   },
   "outputs": [],
   "source": [
    "# The standard network used for all tests\n",
    "\n",
    "torch.manual_seed(111)\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, indim):\n",
    "        super(Network,self).__init__()\n",
    "        self.l1 = nn.Linear(indim,100)\n",
    "        self.l2 = nn.Linear(100,10)\n",
    "        self.l3 = nn.Linear(10,9)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.l1(x))\n",
    "        x = F.leaky_relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "    \n",
    "    # Used in LALR\n",
    "    def penultimate(self, x):\n",
    "        op = F.leaky_relu(self.l1(x))\n",
    "        op = F.leaky_relu(self.l2(op))\n",
    "        return op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "vB5ALlu7uFNn"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Google's Trustscore Implementation : https://github.com/google/TrustScore\n",
    "'''\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "\n",
    "class TrustScore:\n",
    "  \"\"\"\n",
    "    Trust Score: a measure of classifier uncertainty based on nearest neighbors.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, k=10, alpha=0., filtering=\"none\", min_dist=1e-12):\n",
    "    \"\"\"\n",
    "        k and alpha are the tuning parameters for the filtering,\n",
    "        filtering: method of filtering. option are \"none\", \"density\",\n",
    "        \"uncertainty\"\n",
    "        min_dist: some small number to mitigate possible division by 0.\n",
    "    \"\"\"\n",
    "    self.k = k\n",
    "    self.filtering = filtering\n",
    "    self.alpha = alpha\n",
    "    self.min_dist = min_dist\n",
    "\n",
    "  def filter_by_density(self, X):\n",
    "    \"\"\"Filter out points with low kNN density.\n",
    "\n",
    "    Args:\n",
    "    X: an array of sample points.\n",
    "\n",
    "    Returns:\n",
    "    A subset of the array without points in the bottom alpha-fraction of\n",
    "    original points of kNN density.\n",
    "    \"\"\"\n",
    "    kdtree = KDTree(X)\n",
    "    knn_radii = kdtree.query(X, k=self.k)[0][:, -1]\n",
    "    eps = np.percentile(knn_radii, (1 - self.alpha) * 100)\n",
    "    return X[np.where(knn_radii <= eps)[0], :]\n",
    "\n",
    "  def filter_by_uncertainty(self, X, y):\n",
    "    \"\"\"Filter out points with high label disagreement amongst its kNN neighbors.\n",
    "\n",
    "    Args:\n",
    "    X: an array of sample points.\n",
    "\n",
    "    Returns:\n",
    "    A subset of the array without points in the bottom alpha-fraction of\n",
    "    samples with highest disagreement amongst its k nearest neighbors.\n",
    "    \"\"\"\n",
    "    neigh = KNeighborsClassifier(n_neighbors=self.k)\n",
    "    neigh.fit(X, y)\n",
    "    confidence = neigh.predict_proba(X)\n",
    "    cutoff = np.percentile(confidence, self.alpha * 100)\n",
    "    unfiltered_idxs = np.where(confidence >= cutoff)[0]\n",
    "    return X[unfiltered_idxs, :], y[unfiltered_idxs]\n",
    "\n",
    "  def fit(self, X, y):\n",
    "    \"\"\"Initialize trust score precomputations with training data.\n",
    "\n",
    "    WARNING: assumes that the labels are 0-indexed (i.e.\n",
    "    0, 1,..., n_labels-1).\n",
    "\n",
    "    Args:\n",
    "    X: an array of sample points.\n",
    "    y: corresponding labels.\n",
    "    \"\"\"\n",
    "    self.n_labels = np.max(y) + 1\n",
    "    self.kdtrees = [None] * self.n_labels\n",
    "    if self.filtering == \"uncertainty\":\n",
    "      X_filtered, y_filtered = self.filter_by_uncertainty(X, y)\n",
    "    for label in range(self.n_labels):\n",
    "      if self.filtering == \"none\":\n",
    "        X_to_use = X[np.where(y == label)[0]]\n",
    "        self.kdtrees[label] = KDTree(X_to_use)\n",
    "      elif self.filtering == \"density\":\n",
    "        X_to_use = self.filter_by_density(X[np.where(y == label)[0]])\n",
    "        self.kdtrees[label] = KDTree(X_to_use)\n",
    "      elif self.filtering == \"uncertainty\":\n",
    "        X_to_use = X_filtered[np.where(y_filtered == label)[0]]\n",
    "        self.kdtrees[label] = KDTree(X_to_use)\n",
    "\n",
    "      if len(X_to_use) == 0:\n",
    "        print(\n",
    "            \"Filtered too much or missing examples from a label! Please lower \"\n",
    "            \"alpha or check data.\")\n",
    "\n",
    "  def get_score(self, X, y_pred):\n",
    "    \"\"\"Compute the trust scores.\n",
    "\n",
    "    Given a set of points, determines the distance to each class.\n",
    "\n",
    "    Args:\n",
    "    X: an array of sample points.\n",
    "    y_pred: The predicted labels for these points.\n",
    "\n",
    "    Returns:\n",
    "    The trust score, which is ratio of distance to closest class that was not\n",
    "    the predicted class to the distance to the predicted class.\n",
    "    \"\"\"\n",
    "    d = np.tile(None, (X.shape[0], self.n_labels))\n",
    "    for label_idx in range(self.n_labels):\n",
    "      d[:, label_idx] = self.kdtrees[label_idx].query(X, k=2)[0][:, -1]\n",
    "\n",
    "    sorted_d = np.sort(d, axis=1)\n",
    "    d_to_pred = d[range(d.shape[0]), y_pred]\n",
    "    d_to_closest_not_pred = np.where(sorted_d[:, 0] != d_to_pred,\n",
    "                                     sorted_d[:, 0], sorted_d[:, 1])\n",
    "    return d_to_closest_not_pred / (d_to_pred + self.min_dist)\n",
    "\n",
    "\n",
    "class KNNConfidence:\n",
    "  \"\"\"Baseline which uses disagreement to kNN classifier.\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self, k=10):\n",
    "    self.k = k\n",
    "\n",
    "  def fit(self, X, y):\n",
    "    self.kdtree = KDTree(X)\n",
    "    self.y = y\n",
    "\n",
    "  def get_score(self, X, y_pred):\n",
    "    knn_idxs = self.kdtree.query(X, k=self.k)[1]\n",
    "    knn_outputs = self.y[knn_idxs]\n",
    "    return np.mean(\n",
    "        knn_outputs == np.transpose(np.tile(y_pred, (self.k, 1))), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HPZjTvM1uVz_",
    "outputId": "389ae9c8-8047-4b6e-ac08-c90532d98d00"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Datasets/Classification/heart.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-7bee0da92a94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mruns\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     X_train,X_val,y_train,y_val, data_Y, data_X, threshval, all_classes = create_xy(dataset, x_cols, y_col, separator,\n\u001b[1;32m---> 15\u001b[1;33m                                                                                         split_ratio, 0, Scaler,remove_head)\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mall_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_Y\u001b[0m\u001b[1;33m<=\u001b[0m\u001b[0mthreshval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-2-69fa85859c0e>\u001b[0m in \u001b[0;36mcreate_xy\u001b[1;34m(dataset, attribute_columns, target_column, delim, split_ratio, threshold, Scaler, ditch_head)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Dataset Splitter function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mcreate_xy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattribute_columns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_column\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdelim\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msplit_ratio\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mthreshold\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mScaler\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mditch_head\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m         \u001b[0mlines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Datasets/Classification/heart.csv'"
     ]
    }
   ],
   "source": [
    "dataset =  '../Datasets/Classification/heart.csv'\n",
    "x_cols = list(range(13))\n",
    "y_col = 13\n",
    "separator = \",\"\n",
    "remove_head = True\n",
    "split_ratio = 0.2\n",
    "\n",
    "runs = 10\n",
    "\n",
    "avg_conf_res = [[] for i in range(10)]\n",
    "avg_ts_res = [[] for i in range(10)]\n",
    "\n",
    "for r in range(runs):\n",
    "    X_train,X_val,y_train,y_val, data_Y, data_X, threshval, all_classes = create_xy(dataset, x_cols, y_col, separator,\n",
    "                                                                                        split_ratio, 0, Scaler,remove_head)\n",
    "\n",
    "    all_labels = np.where(data_Y<=threshval,0,1)\n",
    "    k = threshval/data_Y.mean()\n",
    "    new_Y = data_Y * k\n",
    "    cmp_Y = (new_Y - threshval)/new_Y.std()\n",
    "    X_cov = torch.Tensor(data_X)\n",
    "    y_cov = torch.Tensor(all_labels)\n",
    "\n",
    "    cov_dataset = data_utils.TensorDataset(X_cov, y_cov)\n",
    "    cov_loader = data_utils.DataLoader(cov_dataset, batch_size = 512, pin_memory=True,shuffle=False,num_workers = 1)\n",
    "\n",
    "    X_train = torch.Tensor(X_train)\n",
    "    y_train = torch.tensor(y_train)\n",
    "    train_dataset = data_utils.TensorDataset(X_train, y_train)\n",
    "    train_loader = data_utils.DataLoader(train_dataset, batch_size =batch_is, pin_memory=True,shuffle=False,num_workers = 1)\n",
    "\n",
    "    indim = X_train.shape[1]\n",
    "    model = Network(indim)\n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr = lr_is)\n",
    "    total_epochs = 20\n",
    "    for epoch in range(total_epochs):\n",
    "      acc_train = train(model,optimizer, train_loader,epoch,False)\n",
    "    prediction = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "      all_preds = [[] for i in range(9)]\n",
    "      test_labels = []\n",
    "      for i,j in cov_loader:\n",
    "        inputs,labels = i.to(device),j.to(device)\n",
    "        op_qs = model(inputs)        \n",
    "        for itemset in op_qs.detach():\n",
    "          for quant in range(9):\n",
    "            all_preds[quant].append(itemset[quant].item())\n",
    "          for lag in labels.reshape(-1,1):\n",
    "            test_labels.append(lag.item())\n",
    "          if itemset[4].item() <= 0:\n",
    "            prediction.append(0)\n",
    "          else:\n",
    "            prediction.append(1)\n",
    "\n",
    "        delta = []\n",
    "        dv_misc = []\n",
    "        dv_correct = []\n",
    "        was_correct = []\n",
    "\n",
    "        totals = [0,0,0,0,0]\n",
    "        misclassifications = [0,0,0,0,0]\n",
    "\n",
    "        for i in range(len(all_preds[0])):\n",
    "          delta_val = 0\n",
    "          base = all_preds[4][i]\n",
    "          while delta_val<5:\n",
    "            left = all_preds[4-delta_val][i]\n",
    "            right = all_preds[4+delta_val][i]\n",
    "            if left<=0<=right:\n",
    "              break\n",
    "            else:\n",
    "              delta_val +=1\n",
    "          delta.append(delta_val/10)\n",
    "          if base<=0:\n",
    "            pred_class = 0\n",
    "          else:\n",
    "            pred_class = 1\n",
    "          if pred_class == test_labels[i]:\n",
    "            was_correct.append(\"^\")\n",
    "            dv_correct.append(delta_val/10)\n",
    "          else:\n",
    "            was_correct.append(\"v\")\n",
    "            dv_misc.append(delta_val/10)\n",
    "            misclassifications[delta_val-1] +=1\n",
    "          totals[delta_val-1] +=1\n",
    "\n",
    "    delta = []\n",
    "    dv_misc = []\n",
    "    dv_correct = []\n",
    "    was_correct = []\n",
    "\n",
    "    totals = [0,0,0,0,0]\n",
    "    misclassifications = [0,0,0,0,0]\n",
    "\n",
    "    for i in range(len(all_preds[0])):\n",
    "      delta_val = 0\n",
    "      base = all_preds[4][i]\n",
    "      while delta_val<5:\n",
    "        left = all_preds[4-delta_val][i]\n",
    "        right = all_preds[4+delta_val][i]\n",
    "        if left<=0<=right:\n",
    "          break\n",
    "        else:\n",
    "          delta_val +=1\n",
    "      delta.append(delta_val/10)\n",
    "      if base<=0:\n",
    "        pred_class = 0\n",
    "      else:\n",
    "        pred_class = 1\n",
    "      if pred_class == test_labels[i]:\n",
    "        dv_correct.append(delta_val/10)\n",
    "      else:\n",
    "        dv_misc.append(delta_val/10)\n",
    "        misclassifications[delta_val-1] +=1\n",
    "      totals[delta_val-1] +=1\n",
    "\n",
    "\n",
    "    trust_model = TrustScore()\n",
    "    trust_model.fit(data_X, all_labels)\n",
    "    trust_scores = trust_model.get_score(data_X, np.array(prediction))\n",
    "\n",
    "    t_rank = np.argsort(trust_scores)\n",
    "    t_score = trust_scores[t_rank]\n",
    "    steps = len(trust_scores)/10\n",
    "    c_score = np.array(delta)[t_rank]\n",
    "\n",
    "    for i in range(0,10):\n",
    "      start = int(steps*i)\n",
    "      end = int(steps*(i+1))\n",
    "      conf_slice = c_score[start: end]\n",
    "      trust_slice = t_score[start:end]\n",
    "      avg_conf_res[i].append(np.mean(conf_slice))\n",
    "      avg_ts_res[i].append(np.mean(trust_slice))\n",
    "\n",
    "    print(\"Run:\",r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VHwbGW-fvxDW",
    "outputId": "e5cb5881-f64c-4a7b-92e0-226477bb5f6d"
   },
   "outputs": [],
   "source": [
    "print(\"Average conf. score per trustscore bin\")\n",
    "\n",
    "print(\"Bin   \", end=\":\")\n",
    "for i in range(10):\n",
    "  print(' Bin ' + str(i+1) + ' ', end= \" | \")\n",
    "print()\n",
    "print(\"Delta \", end=\":\")\n",
    "for i in range(10):\n",
    "  print(' {:.2f}  '.format(np.mean(avg_conf_res[i])), end= \" | \")\n",
    "print()\n",
    "print(\"TS    \", end=\":\")\n",
    "for i in range(10):\n",
    "  print(' {:.2f}  '.format(np.mean(avg_ts_res[i])), end= \" | \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "njJON03W1sWI"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Trustscore_Comparison.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
