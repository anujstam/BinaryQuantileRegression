{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "1caffR-ib-OO",
    "outputId": "d92271f3-2e3b-4efa-86bc-4eed6d9b715f"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data_utils\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning) # Prevents Tanh warning messages \n",
    "\n",
    "Scaler = StandardScaler()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PFwdfw9scABP"
   },
   "outputs": [],
   "source": [
    "def create_xy(dataset, attribute_columns, target_column, delim, split_ratio,ditch_head=False):\n",
    "    with open(dataset, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    if ditch_head:\n",
    "        lines = lines[1:]\n",
    "    X = []\n",
    "    Y = []\n",
    "    for line in lines:\n",
    "        while len(line) > 0 and line[-1] == \"\\n\":\n",
    "            line = line[:len(line)-1]\n",
    "        split_array = line.split(delim)\n",
    "        all_columns = []\n",
    "        for value in split_array:\n",
    "            if value !=\"\" and value !=\" \":\n",
    "                all_columns.append(value)\n",
    "        if len(all_columns)==0:\n",
    "            break\n",
    "        point = []\n",
    "        for i in attribute_columns:\n",
    "            point.append(float(all_columns[i]))\n",
    "        try:\n",
    "            Y.append(float(all_columns[target_column]))\n",
    "            X.append(point)\n",
    "        except:\n",
    "            pass\n",
    "    X_arr = np.asarray(X)\n",
    "    Scaler.fit(X_arr)\n",
    "    X_arr = Scaler.transform(X_arr)\n",
    "    Y_arr = np.asarray(Y)\n",
    "    thresh = np.median(Y_arr)\n",
    "    Y_arr_binary = np.where(Y_arr<=0,0,1)\n",
    "    unique, counts = np.unique(Y_arr_binary, return_counts=True)\n",
    "    x_train, x_test, y_train, y_test = train_test_split(X_arr, Y_arr_binary, test_size = split_ratio)\n",
    "    return x_train, x_test, y_train, y_test, Y_arr, X_arr, Y_arr_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oDjtcohEcRMh"
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(111)\n",
    "class Network(nn.Module):\n",
    "    def __init__(self, indim):\n",
    "        super(Network,self).__init__()\n",
    "        self.l1 = nn.Linear(indim,100)\n",
    "        self.l2 = nn.Linear(100,50)\n",
    "        self.l3 = nn.Linear(50,9)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        x = F.leaky_relu(self.l1(x))\n",
    "        x = F.leaky_relu(self.l2(x))\n",
    "        x = self.l3(x)\n",
    "        return x\n",
    "    \n",
    "    def penultimate(self, x):\n",
    "        op = F.leaky_relu(self.l1(x))\n",
    "        op = F.leaky_relu(self.l2(op))\n",
    "        return op\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mxj5vszgcWZH"
   },
   "outputs": [],
   "source": [
    "# Loss and Accuracy Computation functions\n",
    "\n",
    "def cumLaplaceDistribution(y_pred,mean,standard_deviation,all_qs):\n",
    "    term1 = ((1-all_qs) * (y_pred - mean))/standard_deviation\n",
    "    term1.clamp_(max = 0) # Prevents NaN - Only one of term 1 or 2 is used, whichever is -ve\n",
    "    lesser_term = all_qs * torch.exp(term1)\n",
    "    term2 = (-1.0 * all_qs * (y_pred - mean))/standard_deviation\n",
    "    term2.clamp_(max = 0) # Again, Prevents NaN\n",
    "    greater_term = 1 - ((1-all_qs) * torch.exp(term2))\n",
    "    mean_tensor = torch.ones_like(mean)\n",
    "    y_mask = torch.div(y_pred,mean_tensor)\n",
    "    y_mask[y_pred >= mean] = 1.0\n",
    "    y_mask[y_pred < mean] = 0.0\n",
    "    return ((1 - y_mask) * lesser_term )+  (y_mask * greater_term)\n",
    "\n",
    "\n",
    "def logLikelihoodLoss(y_true,y_pred,mean,standard_deviation,all_qs):\n",
    "    new_pred = y_pred\n",
    "    prob = cumLaplaceDistribution(0.0,mean = new_pred,\n",
    "                                  standard_deviation = standard_deviation,all_qs = all_qs)\n",
    "    prob.clamp_(min = 1e-7,max = 1 - 1e-7)\n",
    "    if_one = y_true * torch.log(1 - prob)\n",
    "    if_zero = (1 - y_true) * torch.log(prob)\n",
    "    final_loss = - 1 * torch.mean(if_one + if_zero)\n",
    "    return final_loss\n",
    "\n",
    "def customLoss(y_true, y_pred, mean, standard_deviation, all_qs, penalty):\n",
    "    ind_losses = []\n",
    "    for i,j in enumerate(all_qs):\n",
    "        single_quantile_loss = logLikelihoodLoss(y_true[:,0],y_pred[:,i] ,\n",
    "                                                 mean, standard_deviation, j)\n",
    "        ind_losses.append(single_quantile_loss)\n",
    "    zero = torch.Tensor([0]).to(device)\n",
    "    dummy1 = y_pred[:,1:] - y_pred[:,:-1]\n",
    "    dummy2 = penalty * torch.mean(torch.max(zero,-1.0 * dummy1))\n",
    "    total_loss  = torch.mean(torch.stack(ind_losses)) +dummy2\n",
    "    return total_loss\n",
    "\n",
    "def customTestPred(y_pred,mean,standard_deviation,all_qs,batch_size = 1):\n",
    "    acc = []\n",
    "    cdfs = []\n",
    "    val = (y_pred - mean)/standard_deviation \n",
    "    \n",
    "    for xx in range(batch_size):\n",
    "        if(y_pred < mean[xx]):\n",
    "            lesser_term = all_qs * torch.exp((1.0 - all_qs) * torch.tensor(val[xx], dtype=torch.double)) \n",
    "            # Typecast above needed for some versions of torch\n",
    "            lesser_term  = 1 - lesser_term\n",
    "            cdfs.append(lesser_term.item())\n",
    "            if(lesser_term.item() >= 0.5):\n",
    "                acc.append([1])\n",
    "            else:\n",
    "                acc.append([0])\n",
    "        \n",
    "        elif(y_pred >= mean[xx]):\n",
    "            greater_term = 1.0 - ((1.0-all_qs) * torch.exp(-1.0 * all_qs * torch.tensor(val[xx], dtype=torch.double)))\n",
    "            # Typecast above needed for some versions of torch\n",
    "            greater_term = 1 - greater_term\n",
    "            cdfs.append(greater_term.item())\n",
    "            if(greater_term.item() >= 0.5):\n",
    "                acc.append([1])\n",
    "            else:\n",
    "                acc.append([0])\n",
    "    return torch.Tensor(acc).to(device).reshape(-1,1),torch.Tensor(cdfs).to(device).reshape(-1,1)\n",
    "\n",
    "def acc_tests(test_preds,test_labels):\n",
    "    test_preds = np.array(test_preds).reshape(-1,1)\n",
    "    test_labels = np.array(test_labels).reshape(-1,1)\n",
    "    cdfs_acc,_ = customTestPred(0,test_preds,standard_deviation = 1,all_qs = torch.Tensor([0.5]),\n",
    "                                batch_size = test_preds.shape[0])\n",
    "\n",
    "    count = 0\n",
    "    for i,j in zip(cdfs_acc,test_labels):\n",
    "        if(i.item() == j[0]):\n",
    "            count += 1\n",
    "    return count/test_labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dhne82Ai21Zz"
   },
   "outputs": [],
   "source": [
    "def lr_schedule_combined_sgd(model, loader, batch_size):\n",
    "    Kz = 0.0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i,j in enumerate(loader):\n",
    "            inputs,labels = j[0],j[1]\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            op = model.penultimate(inputs)\n",
    "            activ = np.linalg.norm(op.detach().cpu().numpy())\n",
    "            if activ > Kz:\n",
    "                Kz = activ\n",
    "    \n",
    "    factor = 1    \n",
    "    K_ = (factor * Kz) / (batch_size)\n",
    "    lr = 1 / K_\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0x2BWMY0cdnW"
   },
   "outputs": [],
   "source": [
    "def train(model,optimizer,loader,epochs, verbose=False):\n",
    "    train_preds_Q = []\n",
    "    train_labels = []\n",
    "    model.train()\n",
    "    \n",
    "    for i,j in enumerate(loader):\n",
    "        inputs,labels = j[0],j[1]\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        op_qs = model(inputs)\n",
    "        lossQ = customLoss(labels.reshape(-1,1),op_qs, mean_is,std_is,all_qs,penalty)\n",
    "        lossQ.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        for lag in op_qs[:,4].detach().reshape(-1,1):\n",
    "            train_preds_Q.append(lag.item())\n",
    "        for lag in labels.reshape(-1,1):\n",
    "            train_labels.append(lag.item())\n",
    "            \n",
    "    acc_is_Q = acc_tests(train_preds_Q,train_labels)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"[%d/%d] Train Acc Q : %f \"%(epochs,total_epochs,acc_is_Q))    \n",
    "    return acc_is_Q\n",
    "\n",
    "def train_adaptive_lr(model,optimizer,loader, epochs, verbose=False):\n",
    "    train_preds_Q = []\n",
    "    train_labels = []\n",
    "    lr_val = lr_schedule_combined_sgd(model, loader, batch_is)\n",
    "    optimizer.param_groups[0]['lr'] = lr_val\n",
    "    model.train()\n",
    "    for i,j in enumerate(loader):\n",
    "        inputs,labels = j[0],j[1]\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        op_qs = model(inputs)\n",
    "        lossQ = customLoss(labels.reshape(-1,1),op_qs, mean_is,std_is,all_qs,penalty)\n",
    "        lossQ.backward()\n",
    "        optimizer.step()\n",
    "        for lag in op_qs[:,4].detach().reshape(-1,1):\n",
    "            train_preds_Q.append(lag.item())\n",
    "        for lag in labels.reshape(-1,1):\n",
    "            train_labels.append(lag.item())\n",
    "    \n",
    "    acc_is_Q = acc_tests(train_preds_Q,train_labels)\n",
    "    if verbose:\n",
    "        print(\"[%d/%d] Train Acc Q : %f \"%(epochs,total_epochs,acc_is_Q))\n",
    "    return acc_is_Q\n",
    "\n",
    "def test(model,loader,epochs,verbose=False):\n",
    "    model.eval()\n",
    "    test_preds_Q = []\n",
    "    test_preds_bce = []\n",
    "    test_labels = []\n",
    "    with torch.no_grad():\n",
    "        for i,j in enumerate(loader):\n",
    "            inputs,labels = j[0],j[1]\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            op_qs = model(inputs)\n",
    "            \n",
    "            for lag in op_qs[:,4].detach().reshape(-1,1):\n",
    "                test_preds_Q.append(lag.item())\n",
    "            for lag in labels.reshape(-1,1):\n",
    "                test_labels.append(lag.item())\n",
    "                \n",
    "    acc_is_Q = acc_tests(test_preds_Q,test_labels)\n",
    "    \n",
    "    if verbose:\n",
    "        print(\"[%d/%d] Test Acc Q : %f  \"%(epochs,total_epochs,acc_is_Q))\n",
    "    return acc_is_Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uWB-KbTHciEu"
   },
   "outputs": [],
   "source": [
    "def quantileCDF(x, tau):\n",
    "    if x>0:\n",
    "        return 1 - tau*np.exp((tau-1)*x)\n",
    "    else:\n",
    "        return (1 - tau)*np.exp(tau*x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IJA6CBG4cJye",
    "outputId": "44610ef4-d303-467f-bd9c-fc43898e74d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch Device: cpu\n"
     ]
    }
   ],
   "source": [
    "batch_is = 64\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.backends.cudnn.deterministic=True\n",
    "print(\"Torch Device:\",device)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "O6KMY7zwcbvo"
   },
   "outputs": [],
   "source": [
    "# General Control Parameters for the Quantile loss. Need not be changed\n",
    "lr_is = 1e-1\n",
    "mean_is = 0\n",
    "std_is = 1\n",
    "penalty = 1\n",
    "alpha = 0.0\n",
    "\n",
    "# Tau tensor\n",
    "all_qs = [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]\n",
    "all_qs = torch.Tensor(all_qs).to(device)\n",
    "all_qs = all_qs.double()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lOROPmdngdkv"
   },
   "outputs": [],
   "source": [
    "# These control the dataset\n",
    "dataset = '../Datasets/Classification/BankNote_Authentication.csv'\n",
    "x_cols = list(range(4))\n",
    "y_col = 4\n",
    "separator = \",\"\n",
    "remove_head = True\n",
    "target_acc = 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "3D6GEeKWcj_f",
    "outputId": "4fae3273-7cbc-4dc6-b521-a5a48815a576"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch time: 0.0253\n",
      "Adapt epoch: 1 0.699 0.433\n",
      "0.9916666666666667 0.9883381924198251\n",
      "\n",
      "Epoch time: 0.0144\n",
      "Fixed epoch: 1 0.640 0.638\n",
      "Fixed epoch: 51 0.852 0.859\n",
      "Fixed epoch: 101 0.904 0.910\n",
      "Fixed epoch: 151 0.944 0.947\n",
      "Fixed epoch: 201 0.957 0.959\n",
      "Fixed epoch: 251 0.967 0.968\n",
      "Fixed epoch: 301 0.973 0.973\n",
      "Fixed epoch: 351 0.975 0.974\n",
      "Fixed epoch: 401 0.976 0.976\n",
      "Fixed epoch: 451 0.976 0.977\n",
      "Fixed epoch: 501 0.977 0.977\n",
      "Fixed epoch: 551 0.980 0.980\n",
      "Fixed epoch: 601 0.983 0.984\n",
      "Fixed epoch: 651 0.983 0.984\n",
      "Fixed epoch: 701 0.983 0.984\n",
      "Fixed epoch: 751 0.983 0.984\n",
      "Fixed epoch: 801 0.983 0.984\n",
      "Fixed epoch: 851 0.983 0.984\n",
      "Fixed epoch: 901 0.988 0.988\n",
      "Fixed epoch: 951 0.988 0.988\n",
      "0.9916666666666667 0.9912536443148688\n"
     ]
    }
   ],
   "source": [
    "X_train,X_val,y_train,y_val, data_Y, data_X, all_classes = create_xy(dataset, x_cols, y_col, separator, 0.3,remove_head)\n",
    "X_train = torch.Tensor(X_train)\n",
    "y_train = torch.Tensor(y_train)\n",
    "X_val = torch.Tensor(X_val)\n",
    "y_val = torch.Tensor(y_val)\n",
    "X_cov = torch.Tensor(data_X)\n",
    "y_cov = torch.Tensor(data_Y)\n",
    "cov_dataset = data_utils.TensorDataset(X_cov, y_cov)\n",
    "cov_loader = data_utils.DataLoader(cov_dataset, batch_size =512, pin_memory=True,shuffle=False,num_workers = 1)\n",
    "\n",
    "train_dataset = data_utils.TensorDataset(X_train, y_train)\n",
    "test_dataset = data_utils.TensorDataset(X_val, y_val)\n",
    "train_loader = data_utils.DataLoader(train_dataset, batch_size =128, pin_memory=True,shuffle=True,num_workers = 1)\n",
    "test_loader = data_utils.DataLoader(test_dataset,batch_size =512,pin_memory=True,shuffle = False,num_workers = 1)\n",
    "\n",
    "indim = X_train.shape[1]\n",
    "\n",
    "model_fixed = Network(indim)\n",
    "model_fixed = model_fixed.to(device)\n",
    "optimizer_fixed = torch.optim.SGD(model_fixed.parameters(), lr = lr_is)\n",
    "\n",
    "model_adapt = Network(indim)\n",
    "model_adapt = model_adapt.to(device)\n",
    "optimizer_adapt = torch.optim.SGD(model_adapt.parameters(), lr = lr_is)\n",
    "\n",
    "acc_train_fixed = 0\n",
    "acc_test_fixed = 0\n",
    "acc_train_adapt = 0\n",
    "acc_test_adapt = 0\n",
    "\n",
    "epoch_count_fixed = 0\n",
    "epoch_count_adapt = 0\n",
    "\n",
    "\n",
    "epoch_count_adapt = 0\n",
    "while (acc_train_adapt < target_acc):\n",
    "    adapt_start = time.time()\n",
    "    acc_train_adapt = train_adaptive_lr(model_adapt,optimizer_adapt,train_loader, epoch_count_adapt)\n",
    "    adapt_end = time.time()\n",
    "    if epoch_count_adapt==0:\n",
    "        print(\"Epoch time: {:.4f}\".format((adapt_end-adapt_start)/60))\n",
    "    acc_test_adapt  = test(model_adapt,cov_loader,epoch_count_adapt)\n",
    "    if epoch_count_adapt %50 == 0:\n",
    "        print(\"Adapt epoch:\", epoch_count_adapt+1, \"{:.3f} {:.3f}\".format(acc_train_adapt, acc_test_adapt))\n",
    "    epoch_count_adapt +=1\n",
    "print(acc_train_adapt, acc_test_adapt)\n",
    "\n",
    "print()\n",
    "\n",
    "while (acc_train_fixed < target_acc) and epoch_count_fixed<5000:\n",
    "    fixed_start = time.time()\n",
    "    acc_train_fixed = train(model_fixed,optimizer_fixed,train_loader, epoch_count_fixed)\n",
    "    fixed_end = time.time()\n",
    "    if epoch_count_fixed==0:\n",
    "        print(\"Epoch time: {:.4f}\".format((fixed_end-fixed_start)/60))\n",
    "    acc_test_fixed  = test(model_fixed,cov_loader,epoch_count_fixed)\n",
    "    if epoch_count_fixed %50 ==0:\n",
    "        print(\"Fixed epoch:\", epoch_count_fixed+1, \"{:.3f} {:.3f}\".format(acc_train_fixed, acc_test_fixed))\n",
    "    epoch_count_fixed +=1\n",
    "print(acc_train_fixed, acc_test_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2jYkruCx58v7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adaptive iteration count: 13\n",
      "Fixed LR Iteration Count: 998\n"
     ]
    }
   ],
   "source": [
    "print(\"Adaptive iteration count:\", epoch_count_adapt)\n",
    "print(\"Fixed LR Iteration Count:\", epoch_count_fixed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "4OraluJCev0l",
    "outputId": "eddf01c6-5d40-4cc2-c681-813a57879c83"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    all_preds = [[] for i in range(9)]\n",
    "    test_labels = []\n",
    "    for i,j in cov_loader:\n",
    "        inputs,labels = i.to(device),j.to(device)\n",
    "        op_qs = model_adapt(inputs)\n",
    "        \n",
    "        for itemset in op_qs.detach():\n",
    "            for quant in range(9):\n",
    "                all_preds[quant].append(itemset[quant].item())\n",
    "            for lag in labels.reshape(-1,1):\n",
    "                test_labels.append(lag.item())\n",
    "    \n",
    "    delta_total = [0,0,0,0,0]\n",
    "    delta_misc = [0,0,0,0,0]\n",
    "    for i in range(len(data_Y)):\n",
    "        start = 4\n",
    "        left = start\n",
    "        right = start\n",
    "        found = False\n",
    "        count = 0\n",
    "        medprob = quantileCDF(all_preds[start][i], 0.5)\n",
    "        while (left>-1 and not found):\n",
    "            q_left = all_preds[left][i]\n",
    "            q_right = all_preds[right][i]\n",
    "            p_left = quantileCDF(q_left, 0.5)\n",
    "            p_right = quantileCDF(q_right, 0.5)\n",
    "            left -=1\n",
    "            right +=1\n",
    "            if (q_left <= 0.5 and q_right>=0.5):\n",
    "                found = True\n",
    "            else:\n",
    "                count +=1\n",
    "        delta_total[count-1] +=1\n",
    "        if (data_Y[i]==0 and medprob<=0.5) or (data_Y[i]==1 and medprob>0.5):\n",
    "            correct_pred = True\n",
    "        else:\n",
    "            correct_pred = False\n",
    "        if not correct_pred:\n",
    "            delta_misc[count-1] +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 50
    },
    "colab_type": "code",
    "id": "fVayIz75eGjL",
    "outputId": "6f49614e-584c-424d-8032-2e74a7c4f17a"
   },
   "outputs": [],
   "source": [
    "delta_header = \"Delta      |\"\n",
    "miscrate     = \"Misc. Rate |\"\n",
    "\n",
    "for i in range(5):\n",
    "    if delta_total[i] != 0:\n",
    "        mr = delta_misc[i]/delta_total[i]\n",
    "    else:\n",
    "        mr = 0\n",
    "    miscrate += \"{:.2f}\".format(mr) + \" | \"\n",
    "    delta_header += \"{:.2f}\".format((0.1*(i+1)))+ \" | \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sGhfWVFw1hPX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta      |0.10 | 0.20 | 0.30 | 0.40 | 0.50 | \n",
      "Misc. Rate |0.72 | 0.17 | 0.07 | 0.02 | 0.00 | \n"
     ]
    }
   ],
   "source": [
    "print(delta_header)\n",
    "print(miscrate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "AdaptiveLR_Comparison.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
